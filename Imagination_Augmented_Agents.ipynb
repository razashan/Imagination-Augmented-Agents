{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Imagination Augmented Agents.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KPxZpC4wcYDU",
        "colab_type": "text"
      },
      "source": [
        "## Actor **Critic**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E4lRBkOwWosP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import torch \n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SfU5nBt8XAAN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class OnPolicy(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(OnePolicy,self).__init__()\n",
        "  def forward(self,x):\n",
        "    raise NotImplementedError\n",
        "  def act(self,x,deterministic=False):\n",
        "    logit, value = self.forward(x)\n",
        "    probs = F.softmax(logit)\n",
        "\n",
        "    if deterministic:\n",
        "      action = probs.max(1)[1]\n",
        "    else:\n",
        "      action = probs.multinomial()\n",
        "    return action\n",
        "  def evaluate_actions(self,x,action):\n",
        "    logit,value = self.forward(x)\n",
        "    probs = F.softmax(logit)\n",
        "    log_probs = F.log_softmax(logit)\n",
        "    action_log_probs = log_probs.gather(1,action)\n",
        "    entropy = -(probs * log_probs).sum(1).mean()\n",
        "    return logit, action_log_probs,value,entropy\n",
        "\n",
        "class ActorCritic(OnPolicy):\n",
        "  def __init__(self,in_shape,num_actions):\n",
        "    super(ActorCritic,self).__init__()\n",
        "    self.in_shape = in_shape\n",
        "    self.features = nn.Sequential(\n",
        "        nn.Conv2d(in_shape[0],16,kernel_size=3,stride=1),\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(16,16,kernel_size=3,stride=2),\n",
        "        nn.ReLU(),\n",
        "    )\n",
        "    self.fc = nn.Sequential(\n",
        "        nn.Linear(self.feature_size(),256),\n",
        "        nn.ReLU(),\n",
        "    )\n",
        "    self.critic = nn.Linear(256,1)\n",
        "    self.actor = nn.Linear(256,num_actions)\n",
        "  def forward(self,x):\n",
        "    x = self.features(x)\n",
        "    x = x.view(x.size(0),-1)\n",
        "    x = self.fc(x)\n",
        "    logit = self.actor(x)\n",
        "    value = self.critic(x)\n",
        "    return logit,value\n",
        "  def feature_size(self):\n",
        "    return self.features(Variable(torch.zeros(1, *self.in_shape))).view(1,-1).size(1)\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hOIyPoBfZr44",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class RolloutStorage(object):\n",
        "    def __init__(self, num_steps, num_envs, state_shape):\n",
        "        self.num_steps = num_steps\n",
        "        self.num_envs  = num_envs\n",
        "        self.states  = torch.zeros(num_steps + 1, num_envs, *state_shape)\n",
        "        self.rewards = torch.zeros(num_steps,     num_envs, 1)\n",
        "        self.masks   = torch.ones(num_steps  + 1, num_envs, 1)\n",
        "        self.actions = torch.zeros(num_steps,     num_envs, 1).long()\n",
        "        self.use_cuda = False\n",
        "            \n",
        "    def cuda(self):\n",
        "        self.use_cuda  = True\n",
        "        self.states    = self.states.cuda()\n",
        "        self.rewards   = self.rewards.cuda()\n",
        "        self.masks     = self.masks.cuda()\n",
        "        self.actions   = self.actions.cuda()\n",
        "        \n",
        "    def insert(self, step, state, action, reward, mask):\n",
        "        self.states[step + 1].copy_(state)\n",
        "        self.actions[step].copy_(action)\n",
        "        self.rewards[step].copy_(reward)\n",
        "        self.masks[step + 1].copy_(mask)\n",
        "        \n",
        "    def after_update(self):\n",
        "        self.states[0].copy_(self.states[-1])\n",
        "        self.masks[0].copy_(self.masks[-1])\n",
        "        \n",
        "    def compute_returns(self, next_value, gamma):\n",
        "        returns   = torch.zeros(self.num_steps + 1, self.num_envs, 1)\n",
        "        if self.use_cuda:\n",
        "            returns = returns.cuda()\n",
        "        returns[-1] = next_value\n",
        "        for step in reversed(range(self.num_steps)):\n",
        "            returns[step] = returns[step + 1] * gamma * self.masks[step + 1] + self.rewards[step]\n",
        "        return returns[:-1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z2rEJSyAciER",
        "colab_type": "text"
      },
      "source": [
        "# **Deep Mind**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hFMNjEAEcmTh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import math\n",
        "\n",
        "STANDARD_MAP = np.array([\n",
        "    [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
        "    [1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1],\n",
        "    [1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1],\n",
        "    [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
        "    [1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1],\n",
        "    [1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1],\n",
        "    [1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1],\n",
        "    [1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1],\n",
        "    [1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1],\n",
        "    [1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1],\n",
        "    [1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1],\n",
        "    [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
        "    [1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1],\n",
        "    [1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1],\n",
        "    [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n",
        "\n",
        "\n",
        "def get_random_position(map_array):\n",
        "  \"\"\"Gets a random available position in a binary map array.\n",
        "  Args:\n",
        "    map_array: numpy array of the map to search an available position on.\n",
        "  Returns:\n",
        "    The chosen random position.\n",
        "  Raises:\n",
        "    ValueError: if there is no available space in the map.\n",
        "  \"\"\"\n",
        "  if map_array.sum() <= 0:\n",
        "    raise ValueError(\"There is no available space in the map.\")\n",
        "  map_dims = len(map_array.shape)\n",
        "  pos = np.zeros(map_dims, dtype=np.int32)\n",
        "  while True:\n",
        "    result = map_array\n",
        "    for i in range(map_dims):\n",
        "      pos[i] = np.random.randint(map_array.shape[i])\n",
        "      result = result[pos[i]]\n",
        "    if result == 0:\n",
        "      break\n",
        "  return pos\n",
        "\n",
        "\n",
        "def update_2d_pos(array_map, pos, action, pos_result):\n",
        "  posv = array_map[pos[0]][pos[1]][action - 1]\n",
        "  pos_result[0] = posv[0]\n",
        "  pos_result[1] = posv[1]\n",
        "  return pos_result\n",
        "\n",
        "\n",
        "def parse_map(map_array):\n",
        "  \"\"\"Parses a map when there are actions: stay, right, up, left, down.\n",
        "  Args:\n",
        "    map_array: 2D numpy array that contains the map.\n",
        "  Returns:\n",
        "    A 3D numpy array (height, width, actions) that contains the resulting state\n",
        "    for a given position + action, and a 2D numpy array (height, width) with the\n",
        "    walls of the map.\n",
        "  Raises:\n",
        "    ValueError: if the map does not contain only zeros and ones.\n",
        "  \"\"\"\n",
        "  act_def = [[0, 0], [0, 1], [-1, 0], [0, -1], [1, 0]]\n",
        "  walls = np.zeros_like(map_array)\n",
        "  new_map_array = []\n",
        "  for i in range(map_array.shape[0]):\n",
        "    new_map_array.append([])\n",
        "    for j in range(map_array.shape[1]):\n",
        "      new_map_array[i].append([])\n",
        "      if map_array[i, j] == 0:\n",
        "        for k in range(len(act_def)):\n",
        "          new_map_array[i][j].append([i + act_def[k][0], j + act_def[k][1]])\n",
        "      elif map_array[i, j] == 1:\n",
        "        for k in range(len(act_def)):\n",
        "          new_map_array[i][j].append([i, j])\n",
        "        walls[i, j] = 1\n",
        "      else:\n",
        "        raise ValueError(\"Option not understood, %d\" % map_array[i, j])\n",
        "      for k in range(len(new_map_array[i][j])):\n",
        "        if map_array[new_map_array[i][j][k][0]][new_map_array[i][j][k][1]] == 1:\n",
        "          new_map_array[i][j][k][0] = i\n",
        "          new_map_array[i][j][k][1] = j\n",
        "  return np.array(new_map_array), walls\n",
        "\n",
        "\n",
        "def observation_as_rgb(obs):\n",
        "  \"\"\"Reduces the 6 channels of `obs` to 3 RGB.\n",
        "  Args:\n",
        "    obs: the observation as a numpy array.\n",
        "  Returns:\n",
        "    An RGB image in the form of a numpy array, with values between 0 and 1.\n",
        "  \"\"\"\n",
        "  height = obs.shape[0]\n",
        "  width = obs.shape[1]\n",
        "  rgb = np.zeros((height, width, 3), dtype=np.float32)\n",
        "  for x in range(height):\n",
        "    for y in range(width):\n",
        "      if obs[x, y, PillEater.PILLMAN] == 1:\n",
        "        rgb[x, y] = [0, 1, 0]\n",
        "      elif obs[x, y, PillEater.GHOSTS] > 0. or obs[x, y, PillEater.GHOSTS_EDIBLE] > 0.:\n",
        "        g = obs[x, y, PillEater.GHOSTS]\n",
        "        ge = obs[x, y, PillEater.GHOSTS_EDIBLE]\n",
        "        rgb[x, y] = [g + ge, ge, 0]\n",
        "      elif obs[x, y, PillEater.PILL] == 1:\n",
        "        rgb[x, y] = [0, 1, 1]\n",
        "      elif obs[x, y, PillEater.FOOD] == 1:\n",
        "        rgb[x, y] = [0, 0, 1]\n",
        "      elif obs[x, y, PillEater.WALLS] == 1:\n",
        "        rgb[x, y] = [1, 1, 1]\n",
        "  return rgb\n",
        "\n",
        "\n",
        "class PillEater(object):\n",
        "\n",
        "  WALLS = 0\n",
        "  FOOD = 1\n",
        "  PILLMAN = 2\n",
        "  GHOSTS = 3\n",
        "  GHOSTS_EDIBLE = 4\n",
        "  PILL = 5\n",
        "  NUM_ACTIONS = 5\n",
        "  MODES = ('regular', 'avoid', 'hunt', 'ambush', 'rush')\n",
        "\n",
        "  def __init__(self, mode, frame_cap=3000):\n",
        "    assert mode in PillEater.MODES\n",
        "    self.nghosts_init = 1\n",
        "    self.ghost_speed_init = 0.5\n",
        "    self.ghost_speed = self.ghost_speed_init\n",
        "    self.ghost_speed_increase = 0.1\n",
        "    self.end_on_collect = False\n",
        "    self.npills = 2\n",
        "    self.pill_duration = 20\n",
        "    self.seed = 123\n",
        "    self.discount = 1\n",
        "    self.stochasticity = 0.05\n",
        "    self.obs_is_rgb = True\n",
        "    self.frame_cap = frame_cap\n",
        "    self.safe_distance = 5\n",
        "    map_array = STANDARD_MAP\n",
        "    self.map, self.walls = parse_map(map_array)\n",
        "    self.map = np.array(self.map)\n",
        "    self.nactions = self.map.shape[2]\n",
        "    self.height = self.map.shape[0]\n",
        "    self.width = self.map.shape[1]\n",
        "    self.reverse_dir = (4, 5, 2, 3)\n",
        "    self.dir_vec = np.array([[0, 1], [-1, 0], [0, -1], [1, 0]])\n",
        "    self.world_state = dict(\n",
        "        pillman=self._make_pillman(),\n",
        "        ghosts=[],\n",
        "        food=np.zeros(shape=(self.height, self.width), dtype=np.float32),\n",
        "        pills=[None] * self.npills,\n",
        "        power=0\n",
        "    )\n",
        "    self.nplanes = 6\n",
        "    self.image = np.zeros(\n",
        "        shape=(self.height, self.width, self.nplanes), dtype=np.float32)\n",
        "    self.color_image = np.zeros(shape=(3, self.height, self.width),\n",
        "                                dtype=np.float32)\n",
        "    self.frame = 0\n",
        "    self.reward = 0.\n",
        "    self.pcontinue = 1.\n",
        "    self._init_level(1)\n",
        "    self._make_image()\n",
        "    self.mode = mode\n",
        "    self.timer = 0\n",
        "    if self.mode == 'regular':\n",
        "      self.step_reward = 0\n",
        "      self.food_reward = 1\n",
        "      self.big_pill_reward = 2\n",
        "      self.ghost_hunt_reward = 5\n",
        "      self.ghost_death_reward = 0\n",
        "      self.all_pill_terminate = False\n",
        "      self.all_ghosts_terminate = False\n",
        "      self.all_food_terminate = True\n",
        "      self.timer_terminate = -1\n",
        "    elif self.mode == 'avoid':\n",
        "      self.step_reward = 0.1\n",
        "      self.food_reward = -0.1\n",
        "      self.big_pill_reward = -5\n",
        "      self.ghost_hunt_reward = -10\n",
        "      self.ghost_death_reward = -20\n",
        "      self.all_pill_terminate = False\n",
        "      self.all_ghosts_terminate = False\n",
        "      self.all_food_terminate = True\n",
        "      self.timer_terminate = 128\n",
        "    elif self.mode == 'hunt':\n",
        "      self.step_reward = 0\n",
        "      self.food_reward = 0\n",
        "      self.big_pill_reward = 1\n",
        "      self.ghost_hunt_reward = 10\n",
        "      self.ghost_death_reward = -20\n",
        "      self.all_pill_terminate = False\n",
        "      self.all_ghosts_terminate = True\n",
        "      self.all_food_terminate = False\n",
        "      self.timer_terminate = -1\n",
        "    elif self.mode == 'ambush':\n",
        "      self.step_reward = 0\n",
        "      self.food_reward = -0.1\n",
        "      self.big_pill_reward = 0\n",
        "      self.ghost_hunt_reward = 10\n",
        "      self.ghost_death_reward = -20\n",
        "      self.all_pill_terminate = False\n",
        "      self.all_ghosts_terminate = True\n",
        "      self.all_food_terminate = False\n",
        "      self.timer_terminate = -1\n",
        "    elif self.mode == 'rush':\n",
        "      self.step_reward = 0\n",
        "      self.food_reward = -0.1\n",
        "      self.big_pill_reward = 10\n",
        "      self.ghost_hunt_reward = 0\n",
        "      self.ghost_death_reward = 0\n",
        "      self.all_pill_terminate = True\n",
        "      self.all_ghosts_terminate = False\n",
        "      self.all_food_terminate = False\n",
        "      self.timer_terminate = -1\n",
        "\n",
        "  def _make_pillman(self):\n",
        "    return self._make_actor(0)\n",
        "\n",
        "  def _make_enemy(self):\n",
        "    return self._make_actor(self.safe_distance)\n",
        "\n",
        "  def _make_actor(self, safe_distance):\n",
        "    \"\"\"Creates an actor.\n",
        "    An actor is a `ConfigDict` with a positions `pos` and a direction `dir`.\n",
        "    The position is an array with two elements, the height and width. The\n",
        "    direction is an integer representing the direction faced by the actor.\n",
        "    Args:\n",
        "      safe_distance: a `float`. The minimum distance from Pillman.\n",
        "    Returns:\n",
        "      A `ConfigDict`.\n",
        "    \"\"\"\n",
        "    actor = {}\n",
        "    if safe_distance > 0:\n",
        "      occupied_map = np.copy(self.walls)\n",
        "\n",
        "      from_ = (self.world_state['pillman']['pos'] - np.array(\n",
        "          [self.safe_distance, self.safe_distance]))\n",
        "      to = (self.world_state['pillman']['pos'] + np.array(\n",
        "          [self.safe_distance, self.safe_distance]))\n",
        "      from_[0] = max(from_[0], 1)\n",
        "      from_[1] = max(from_[1], 1)\n",
        "      to[0] = min(to[0], occupied_map.shape[0])\n",
        "      to[1] = min(to[1], occupied_map.shape[1])\n",
        "\n",
        "      occupied_map[from_[0]:to[0], from_[1]:to[1]] = 1\n",
        "\n",
        "      actor['pos'] = get_random_position(occupied_map)\n",
        "      actor['dir'] = np.random.randint(4)\n",
        "    else:\n",
        "      actor['pos'] = get_random_position(self.walls)\n",
        "      actor['dir'] = np.random.randint(4)\n",
        "\n",
        "    return actor\n",
        "\n",
        "  def _make_pill(self):\n",
        "    pill = dict(\n",
        "        pos=get_random_position(self.walls)\n",
        "    )\n",
        "    return pill\n",
        "\n",
        "  def _init_level(self, level):\n",
        "    \"\"\"Initialises the level.\"\"\"\n",
        "    self.level = level\n",
        "    self._fill_food(self.walls, self.world_state['food'])\n",
        "    self.world_state['pills'] = [self._make_pill() for _ in range(self.npills)]\n",
        "    self.world_state['pillman']['pos'] = get_random_position(self.walls)\n",
        "\n",
        "    self.nghosts = int(self.nghosts_init + math.floor((level - 1) / 2))\n",
        "    self.world_state['ghosts'] = [self._make_enemy() for _ in range(self.nghosts)]\n",
        "    self.world_state['power'] = 0\n",
        "\n",
        "    self.ghost_speed = (\n",
        "        self.ghost_speed_init + self.ghost_speed_increase * (level - 1))\n",
        "    self.timer = 0\n",
        "\n",
        "  def _fill_food(self, walls, food):\n",
        "    food.fill(-1)\n",
        "    food *= walls\n",
        "    food += 1\n",
        "    self.nfood = food.sum()\n",
        "\n",
        "  def _get_food(self, posx, posy):\n",
        "    self.reward += self.food_reward\n",
        "    self.world_state['food'][posx][posy] = 0\n",
        "    self.nfood -= 1\n",
        "    if self.nfood == 0 and self.all_food_terminate:\n",
        "      self._init_level(self.level + 1)\n",
        "\n",
        "  def _get_pill(self, pill_index):\n",
        "    self.world_state['pills'].pop(pill_index)\n",
        "    self.reward += self.big_pill_reward\n",
        "    self.world_state['power'] = self.pill_duration\n",
        "    if (not self.world_state['pills']) and self.all_pill_terminate:\n",
        "      self._init_level(self.level + 1)\n",
        "\n",
        "  def _kill_ghost(self, ghost_index):\n",
        "    self.world_state['ghosts'].pop(ghost_index)\n",
        "    self.reward += self.ghost_hunt_reward\n",
        "    if (not self.world_state['ghosts']) and self.all_ghosts_terminate:\n",
        "      self._init_level(self.level + 1)\n",
        "\n",
        "  def _die_by_ghost(self):\n",
        "    self.reward += self.ghost_death_reward\n",
        "    self.pcontinue = 0\n",
        "\n",
        "  def _move_pillman(self, action):\n",
        "    \"\"\"Moves Pillman following the action in the proto `action_proto`.\"\"\"\n",
        "    action += 1  # our code is 1 based\n",
        "    pos = self.world_state['pillman']['pos']\n",
        "    pillman = self.world_state['pillman']\n",
        "    update_2d_pos(self.map, pos, action, pos)\n",
        "    if self.world_state['food'][pos[0]][pos[1]] == 1:\n",
        "      self._get_food(pos[0], pos[1])\n",
        "    for i, pill in enumerate(self.world_state['pills']):\n",
        "      pos = pill['pos']\n",
        "      if pos[0] == pillman['pos'][0] and pos[1] == pillman['pos'][1]:\n",
        "        self._get_pill(i)\n",
        "        break\n",
        "\n",
        "  def _move_ghost(self, ghost):\n",
        "    \"\"\"Moves the given ghost.\"\"\"\n",
        "    pos = ghost['pos']\n",
        "    new_pos = np.zeros(shape=(2,), dtype=np.float32)\n",
        "    pillman = self.world_state['pillman']\n",
        "    available = []\n",
        "    for i in range(2, self.nactions + 1):\n",
        "      update_2d_pos(self.map, pos, i, new_pos)\n",
        "      if pos[0] != new_pos[0] or pos[1] != new_pos[1]:\n",
        "        available.append(i)\n",
        "    n_available = len(available)\n",
        "    if n_available == 1:\n",
        "      ghost['dir'] = available[0]\n",
        "    elif n_available == 2:\n",
        "      if ghost['dir'] not in available:\n",
        "        if self.reverse_dir[ghost['dir'] - 2] == available[0]:\n",
        "          ghost['dir'] = available[1]\n",
        "        else:\n",
        "          ghost['dir'] = available[0]\n",
        "    else:\n",
        "      rev_dir = self.reverse_dir[ghost['dir'] - 2]\n",
        "      for i in range(n_available):\n",
        "        if available[i] == rev_dir:\n",
        "          available.pop(i)\n",
        "          n_available -= 1\n",
        "          break\n",
        "      prods = np.zeros(n_available, dtype=np.float32)\n",
        "      x = np.array(\n",
        "          [pillman['pos'][0] - pos[0], pillman['pos'][1] - pos[1]], dtype=np.float32)\n",
        "      norm = np.linalg.norm(x)\n",
        "      if norm > 0:\n",
        "        x *= 1. / norm\n",
        "        for i in range(n_available):\n",
        "          prods[i] = np.dot(x, self.dir_vec[available[i] - 2])\n",
        "        if self.world_state['power'] == 0:\n",
        "          if self.stochasticity > np.random.uniform():\n",
        "            j = np.random.randint(n_available)\n",
        "          else:\n",
        "            # move towards pillman:\n",
        "            j = np.argmax(prods)\n",
        "        else:\n",
        "          # run away from pillman:\n",
        "          j = np.argmin(prods)\n",
        "        ghost['dir'] = available[j]\n",
        "    update_2d_pos(self.map, pos, ghost['dir'], pos)\n",
        "\n",
        "  def _make_image(self):\n",
        "    \"\"\"Represents world in a `height x width x 6` `Tensor`.\"\"\"\n",
        "    self.image.fill(0)\n",
        "    self.image[:, :, PillEater.WALLS] = self.walls\n",
        "    self.image[:, :, PillEater.FOOD] = self.world_state['food']\n",
        "    self.image[self.world_state['pillman']['pos'][0], self.world_state['pillman']['pos'][1],\n",
        "               PillEater.PILLMAN] = 1\n",
        "    for ghost in self.world_state['ghosts']:\n",
        "      edibility = self.world_state['power'] / float(self.pill_duration)\n",
        "      self.image[ghost['pos'][0], ghost['pos'][1], PillEater.GHOSTS] = 1. - edibility\n",
        "      self.image[ghost['pos'][0], ghost['pos'][1], PillEater.GHOSTS_EDIBLE] = edibility\n",
        "    for pill in self.world_state['pills']:\n",
        "      self.image[pill['pos'][0], pill['pos'][1], PillEater.PILL] = 1\n",
        "    return self.image\n",
        "\n",
        "  def start(self):\n",
        "    \"\"\"Starts a new episode.\"\"\"\n",
        "    self.frame = 0\n",
        "    self._init_level(1)\n",
        "    self.reward = 0\n",
        "    self.pcontinue = 1\n",
        "    self.ghost_speed = self.ghost_speed_init\n",
        "    return self._make_image(), self.reward, self.pcontinue\n",
        "\n",
        "  def step(self, action):\n",
        "    \"\"\"Advances environment one time-step following the given action.\"\"\"\n",
        "    self.frame += 1\n",
        "    pillman = self.world_state['pillman']\n",
        "    self.pcontinue = self.discount\n",
        "    self.reward = self.step_reward\n",
        "    self.timer += 1\n",
        "    # Update world state\n",
        "    self.world_state['power'] = max(0, self.world_state['power']-1)\n",
        "\n",
        "    # move pillman\n",
        "    self._move_pillman(action)\n",
        "\n",
        "    for i, ghost in enumerate(self.world_state['ghosts']):\n",
        "      # first check if pillman went onto a ghost\n",
        "      pos = ghost['pos']\n",
        "      if pos[0] == pillman['pos'][0] and pos[1] == pillman['pos'][1]:\n",
        "        if self.world_state['power'] == 0:\n",
        "          self._die_by_ghost()\n",
        "        else:\n",
        "          self._kill_ghost(i)\n",
        "          break\n",
        "      # Then move ghosts\n",
        "      speed = self.ghost_speed\n",
        "      if self.world_state['power'] != 0:\n",
        "        speed *= 0.5\n",
        "      if np.random.uniform() < speed:\n",
        "        self._move_ghost(ghost)\n",
        "        pos = ghost['pos']\n",
        "        # check if ghost went onto pillman\n",
        "        if pos[0] == pillman['pos'][0] and pos[1] == pillman['pos'][1]:\n",
        "          if self.world_state['power'] == 0:\n",
        "            self._die_by_ghost()\n",
        "          else:\n",
        "            self._kill_ghost(i)\n",
        "            # assume you can only eat one ghost per turn:\n",
        "            break\n",
        "    self._make_image()\n",
        "\n",
        "    # Check if level over\n",
        "    if self.timer == self.timer_terminate:\n",
        "      self._init_level(self.level + 1)\n",
        "\n",
        "    # Check if framecap reached\n",
        "    if self.frame_cap > 0 and self.frame >= self.frame_cap:\n",
        "      self.pcontinue = 0\n",
        "\n",
        "  def observation(self, agent_id=0):\n",
        "    return (self.reward,\n",
        "            self.pcontinue,\n",
        "            observation_as_rgb(self.image))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W9fqDYNqDN7F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    def __init__(self, in_shape, n1, n2, n3):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        \n",
        "        self.in_shape = in_shape\n",
        "        self.n1 = n1\n",
        "        self.n2 = n2\n",
        "        self.n3 = n3\n",
        "        \n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=in_shape[1:])\n",
        "        self.conv1 = nn.Sequential(\n",
        "            nn.Conv2d(in_shape[0] * 2, n1, kernel_size=1, stride=2, padding=6),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(n1, n1, kernel_size=10, stride=1, padding=(5, 6)),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "        self.conv2 = nn.Sequential(\n",
        "            nn.Conv2d(in_shape[0] * 2, n2, kernel_size=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(n2, n2, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "        self.conv3 = nn.Sequential(\n",
        "            nn.Conv2d(n1 + n2,  n3, kernel_size=1),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        \n",
        "    def forward(self, inputs):\n",
        "        x = self.pool_and_inject(inputs)\n",
        "        x = torch.cat([self.conv1(x), self.conv2(x)], 1)\n",
        "        x = self.conv3(x)\n",
        "        x = torch.cat([x, inputs], 1)\n",
        "        return x\n",
        "    \n",
        "    def pool_and_inject(self, x):\n",
        "        pooled     = self.maxpool(x)\n",
        "        tiled      = pooled.expand((x.size(0),) + self.in_shape)\n",
        "        out        = torch.cat([tiled, x], 1)\n",
        "        return out\n",
        "    \n",
        "    \n",
        "class EnvModel(nn.Module):\n",
        "    def __init__(self, in_shape, num_pixels, num_rewards):\n",
        "        super(EnvModel, self).__init__()\n",
        "        \n",
        "        width  = in_shape[1]\n",
        "        height = in_shape[2]\n",
        "        \n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(8, 64, kernel_size=1),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        \n",
        "        self.basic_block1 = BasicBlock((64, width, height), 16, 32, 64)\n",
        "        self.basic_block2 = BasicBlock((128, width, height), 16, 32, 64)\n",
        "        \n",
        "        self.image_conv = nn.Sequential(\n",
        "            nn.Conv2d(192, 256, kernel_size=1),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.image_fc = nn.Linear(256, num_pixels)\n",
        "        \n",
        "        self.reward_conv = nn.Sequential(\n",
        "            nn.Conv2d(192, 64, kernel_size=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 64, kernel_size=1),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.reward_fc    = nn.Linear(64 * width * height, num_rewards)\n",
        "        \n",
        "    def forward(self, inputs):\n",
        "        batch_size = inputs.size(0)\n",
        "        \n",
        "        x = self.conv(inputs)\n",
        "        x = self.basic_block1(x)\n",
        "        x = self.basic_block2(x)\n",
        "        \n",
        "        image = self.image_conv(x)\n",
        "        image = image.permute(0, 2, 3, 1).contiguous().view(-1, 256)\n",
        "        image = self.image_fc(image)\n",
        "\n",
        "        reward = self.reward_conv(x)\n",
        "        reward = reward.view(batch_size, -1)\n",
        "        reward = self.reward_fc(reward)\n",
        "        \n",
        "        return image, reward"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y4vzimGlhesu",
        "colab_type": "text"
      },
      "source": [
        "# **Mini-PacMan**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vyhjd0WisqZ-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gym\n",
        "from gym import spaces\n",
        "\n",
        "\n",
        "class MiniPacman:\n",
        "    def __init__(self, mode, frame_cap):\n",
        "        self.mode      = mode\n",
        "        self.frame_cap = frame_cap\n",
        "        \n",
        "        self.env = PillEater(mode=mode, frame_cap=frame_cap)\n",
        "        \n",
        "        self.action_space      = spaces.Discrete(5)\n",
        "        self.observation_space = spaces.Box(low=0, high=1.0, shape=(3, 15, 19))\n",
        "\n",
        "    def step(self, action):\n",
        "        self.env.step(action)\n",
        "        env_reward, env_pcontinue, env_frame = self.env.observation()\n",
        "        self.done = env_pcontinue != 1\n",
        "        env_frame = env_frame.transpose(2, 0, 1)\n",
        "        return env_frame, env_reward, self.done, {}\n",
        "\n",
        "    def reset(self):\n",
        "        image, _, _ = self.env.start()\n",
        "        image = observation_as_rgb(image)\n",
        "        self.done = False\n",
        "        image = image.transpose(2, 0, 1)\n",
        "        return image"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dU1qhmOpumGu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from multiprocessing import Process, Pipe\n",
        "\n",
        "def worker(remote, parent_remote, env_fn_wrapper):\n",
        "    parent_remote.close()\n",
        "    env = env_fn_wrapper.x()\n",
        "    while True:\n",
        "        cmd, data = remote.recv()\n",
        "        if cmd == 'step':\n",
        "            ob, reward, done, info = env.step(data)\n",
        "            if done:\n",
        "                ob = env.reset()\n",
        "            remote.send((ob, reward, done, info))\n",
        "        elif cmd == 'reset':\n",
        "            ob = env.reset()\n",
        "            remote.send(ob)\n",
        "        elif cmd == 'reset_task':\n",
        "            ob = env.reset_task()\n",
        "            remote.send(ob)\n",
        "        elif cmd == 'close':\n",
        "            remote.close()\n",
        "            break\n",
        "        elif cmd == 'get_spaces':\n",
        "            remote.send((env.observation_space, env.action_space))\n",
        "        else:\n",
        "            raise NotImplementedError\n",
        "\n",
        "class VecEnv(object):\n",
        "    \"\"\"\n",
        "    An abstract asynchronous, vectorized environment.\n",
        "    \"\"\"\n",
        "    def __init__(self, num_envs, observation_space, action_space):\n",
        "        self.num_envs = num_envs\n",
        "        self.observation_space = observation_space\n",
        "        self.action_space = action_space\n",
        "\n",
        "    def reset(self):\n",
        "        \"\"\"\n",
        "        Reset all the environments and return an array of\n",
        "        observations, or a tuple of observation arrays.\n",
        "        If step_async is still doing work, that work will\n",
        "        be cancelled and step_wait() should not be called\n",
        "        until step_async() is invoked again.\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "    def step_async(self, actions):\n",
        "        \"\"\"\n",
        "        Tell all the environments to start taking a step\n",
        "        with the given actions.\n",
        "        Call step_wait() to get the results of the step.\n",
        "        You should not call this if a step_async run is\n",
        "        already pending.\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "    def step_wait(self):\n",
        "        \"\"\"\n",
        "        Wait for the step taken with step_async().\n",
        "        Returns (obs, rews, dones, infos):\n",
        "         - obs: an array of observations, or a tuple of\n",
        "                arrays of observations.\n",
        "         - rews: an array of rewards\n",
        "         - dones: an array of \"episode done\" booleans\n",
        "         - infos: a sequence of info objects\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "    def close(self):\n",
        "        \"\"\"\n",
        "        Clean up the environments' resources.\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "    def step(self, actions):\n",
        "        self.step_async(actions)\n",
        "        return self.step_wait()\n",
        "\n",
        "    \n",
        "class CloudpickleWrapper(object):\n",
        "    \"\"\"\n",
        "    Uses cloudpickle to serialize contents (otherwise multiprocessing tries to use pickle)\n",
        "    \"\"\"\n",
        "    def __init__(self, x):\n",
        "        self.x = x\n",
        "    def __getstate__(self):\n",
        "        import cloudpickle\n",
        "        return cloudpickle.dumps(self.x)\n",
        "    def __setstate__(self, ob):\n",
        "        import pickle\n",
        "        self.x = pickle.loads(ob)\n",
        "\n",
        "class SubprocVecEnv(VecEnv):\n",
        "    def __init__(self, env_fns, spaces=None):\n",
        "        \"\"\"\n",
        "        envs: list of gym environments to run in subprocesses\n",
        "        \"\"\"\n",
        "        self.waiting = False\n",
        "        self.closed = False\n",
        "        nenvs = len(env_fns)\n",
        "        self.nenvs = nenvs\n",
        "        self.remotes, self.work_remotes = zip(*[Pipe() for _ in range(nenvs)])\n",
        "        self.ps = [Process(target=worker, args=(work_remote, remote, CloudpickleWrapper(env_fn)))\n",
        "            for (work_remote, remote, env_fn) in zip(self.work_remotes, self.remotes, env_fns)]\n",
        "        for p in self.ps:\n",
        "            p.daemon = True # if the main process crashes, we should not cause things to hang\n",
        "            p.start()\n",
        "        for remote in self.work_remotes:\n",
        "            remote.close()\n",
        "\n",
        "        self.remotes[0].send(('get_spaces', None))\n",
        "        observation_space, action_space = self.remotes[0].recv()\n",
        "        VecEnv.__init__(self, len(env_fns), observation_space, action_space)\n",
        "\n",
        "    def step_async(self, actions):\n",
        "        for remote, action in zip(self.remotes, actions):\n",
        "            remote.send(('step', action))\n",
        "        self.waiting = True\n",
        "\n",
        "    def step_wait(self):\n",
        "        results = [remote.recv() for remote in self.remotes]\n",
        "        self.waiting = False\n",
        "        obs, rews, dones, infos = zip(*results)\n",
        "        return np.stack(obs), np.stack(rews), np.stack(dones), infos\n",
        "\n",
        "    def reset(self):\n",
        "        for remote in self.remotes:\n",
        "            remote.send(('reset', None))\n",
        "        return np.stack([remote.recv() for remote in self.remotes])\n",
        "\n",
        "    def reset_task(self):\n",
        "        for remote in self.remotes:\n",
        "            remote.send(('reset_task', None))\n",
        "        return np.stack([remote.recv() for remote in self.remotes])\n",
        "\n",
        "    def close(self):\n",
        "        if self.closed:\n",
        "            return\n",
        "        if self.waiting:\n",
        "            for remote in self.remotes:            \n",
        "                remote.recv()\n",
        "        for remote in self.remotes:\n",
        "            remote.send(('close', None))\n",
        "        for p in self.ps:\n",
        "            p.join()\n",
        "            self.closed = True\n",
        "            \n",
        "    def __len__(self):\n",
        "        return self.nenvs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "viSpyvJ2utid",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from IPython.display import clear_output\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jPOehA61u01q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def displayImage(image, step, reward):\n",
        "    s = \"step\" + str(step) + \" reward \" + str(reward)\n",
        "    plt.title(s)\n",
        "    plt.imshow(image)\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WMxkax0Gu28L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "keys = {\n",
        "    'w': 2,\n",
        "    'd': 1,\n",
        "    'a': 3,\n",
        "    's': 4,\n",
        "    ' ': 0\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UuuP330vu5Wt",
        "colab_type": "code",
        "outputId": "95940b77-98b0-49b6-b3ae-c78f182ec923",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 815
        }
      },
      "source": [
        "MODES = ('regular', 'avoid', 'hunt', 'ambush', 'rush')\n",
        "frame_cap = 1000\n",
        "\n",
        "mode = 'rush'\n",
        "\n",
        "env = MiniPacman(mode, 1000)\n",
        "\n",
        "state = env.reset()\n",
        "done = False\n",
        "\n",
        "total_reward = 0\n",
        "step = 1\n",
        "\n",
        "displayImage(state.transpose(1, 2, 0), step, total_reward)\n",
        "\n",
        "while not done:\n",
        "    x =input()\n",
        "    clear_output()\n",
        "    try:\n",
        "        keys[x]\n",
        "    except:\n",
        "        print(\"Only 'w' 'a' 'd' 's'\")\n",
        "        continue\n",
        "    action = keys[x]\n",
        "    \n",
        "    next_state, reward, done, _ = env.step(action)\n",
        "    total_reward += reward\n",
        "    displayImage(next_state.transpose(1, 2, 0), step, total_reward)\n",
        "    step += 1"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAATUAAAEICAYAAAA++2N3AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    729\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 730\u001b[0;31m                 \u001b[0mident\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdin_socket\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    731\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/jupyter_client/session.py\u001b[0m in \u001b[0;36mrecv\u001b[0;34m(self, socket, mode, content, copy)\u001b[0m\n\u001b[1;32m    802\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 803\u001b[0;31m             \u001b[0mmsg_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_multipart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    804\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mzmq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZMQError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/zmq/sugar/socket.py\u001b[0m in \u001b[0;36mrecv_multipart\u001b[0;34m(self, flags, copy, track)\u001b[0m\n\u001b[1;32m    465\u001b[0m         \"\"\"\n\u001b[0;32m--> 466\u001b[0;31m         \u001b[0mparts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrack\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrack\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    467\u001b[0m         \u001b[0;31m# have first part already, only loop while more to receive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket._recv_copy\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/zmq/backend/cython/checkrc.pxd\u001b[0m in \u001b[0;36mzmq.backend.cython.checkrc._check_rc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-9f910b625e17>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0mclear_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    703\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 705\u001b[0;31m             \u001b[0mpassword\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    706\u001b[0m         )\n\u001b[1;32m    707\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    733\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    734\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 735\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    736\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    737\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fFZ1jRQEu9CC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torch.autograd as autograd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rVwWhszo5_o4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from IPython.display import clear_output\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xk6HhQ9r6Pqi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "USE_CUDA  = torch.cuda.is_available()\n",
        "Variable = lambda *args, **kwargs: autograd.Variable(*args,**kwargs).cuda() if USE_CUDA else autograd.Variable(*args,**kwargs)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CUcig3pJ6q6Z",
        "colab_type": "text"
      },
      "source": [
        "# **Actor Critic Algorithm**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eFmX39n_6x4R",
        "colab_type": "text"
      },
      "source": [
        "# **Abstract A2C class**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ak4Q2CKY63gr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class OnPolicy(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(OnPolicy,self).__init__()\n",
        "  def forward(self,x):\n",
        "    raise NotImplementedError\n",
        "  def act(self,x,deterministic=False):\n",
        "    logit,value = self.forward(x)\n",
        "    probs = F.softmax(logit)\n",
        "\n",
        "    if deterministic:\n",
        "      action = probs.max(1)[1]\n",
        "    else:\n",
        "      action = probs.multinomial()\n",
        "    return action\n",
        "  def evaluate_actions(self,x,action):\n",
        "    logit,value = self.forward(x)\n",
        "    probs = F.softmax(logit)\n",
        "    log_probs = F.log_softmax(logit)\n",
        "    action_log_probs = log_probs.gather(1,action)\n",
        "    entropy = -(probs * log_probs).sum(1).mean()\n",
        "    return logit,action_log_probs, value,entropy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ABDgoY6x8WPx",
        "colab_type": "text"
      },
      "source": [
        "# **Neural Network Architecture fot A2C**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RPqT2Ilr8hna",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ActorCritic(OnPolicy):\n",
        "  def __init__(self,in_shape,num_actions):\n",
        "    super(ActorCritic,self).__init__()\n",
        "    self.in_shape = in_shape\n",
        "    self.features = nn.Sequential(\n",
        "        nn.Conv2d(in_shape[0],16,kernel_size=3,stride=1),\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(16,16,kernel_size=3,stride=2),\n",
        "        nn.ReLU(),\n",
        "    )\n",
        "    self.fc = nn.Sequential(\n",
        "        nn.Linear(self.feature_size(),256),\n",
        "        nn.ReLU(),\n",
        "    )\n",
        "    self.critic = nn.Linear(256,1)\n",
        "    self.actor = nn.Linear(256,num_actions)\n",
        "\n",
        "  def forward(self,x):\n",
        "    x = self.features(x)\n",
        "    x = x.view(x.size(0),-1)\n",
        "    x = self.fc(x)\n",
        "    logit = self.actor(x)\n",
        "    value = self.critic(x)\n",
        "    return value, logit\n",
        "  def feature_size(self):\n",
        "    return self.features(autograd.Variable(torch.zeros(1, *self.in_shape))).view(1,-1).size(1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2DCKYT4Z-0j4",
        "colab_type": "text"
      },
      "source": [
        "# **Simple Class to Save experience for A2C critici**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6pko3xYA-8Pm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class RolloutStorage(object):\n",
        "    def __init__(self, num_steps, num_envs, state_shape):\n",
        "        self.num_steps = num_steps\n",
        "        self.num_envs  = num_envs\n",
        "        self.states  = torch.zeros(num_steps + 1, num_envs, *state_shape)\n",
        "        self.rewards = torch.zeros(num_steps,     num_envs, 1)\n",
        "        self.masks   = torch.ones(num_steps  + 1, num_envs, 1)\n",
        "        self.actions = torch.zeros(num_steps,     num_envs, 1).long()\n",
        "        self.use_cuda = False\n",
        "            \n",
        "    def cuda(self):\n",
        "        self.use_cuda  = True\n",
        "        self.states    = self.states.cuda()\n",
        "        self.rewards   = self.rewards.cuda()\n",
        "        self.masks     = self.masks.cuda()\n",
        "        self.actions   = self.actions.cuda()\n",
        "        \n",
        "    def insert(self, step, state, action, reward, mask):\n",
        "        self.states[step + 1].copy_(state)\n",
        "        self.actions[step].copy_(action)\n",
        "        self.rewards[step].copy_(reward)\n",
        "        self.masks[step + 1].copy_(mask)\n",
        "        \n",
        "    def after_update(self):\n",
        "        self.states[0].copy_(self.states[-1])\n",
        "        self.masks[0].copy_(self.masks[-1])\n",
        "        \n",
        "    def compute_returns(self, next_value, gamma):\n",
        "        returns   = torch.zeros(self.num_steps + 1, self.num_envs, 1)\n",
        "        if self.use_cuda:\n",
        "            returns = returns.cuda()\n",
        "        returns[-1] = next_value\n",
        "        for step in reversed(range(self.num_steps)):\n",
        "            returns[step] = returns[step + 1] * gamma * self.masks[step + 1] + self.rewards[step]\n",
        "        return returns[:-1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vRUs27SiB6iK",
        "colab_type": "text"
      },
      "source": [
        "# **Creating Environment**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r0enTo7uB_ab",
        "colab_type": "code",
        "outputId": "50e453f4-e8c3-48ac-8b46-559972da009d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 586
        }
      },
      "source": [
        "mode = \"regular\"\n",
        "num_envs = 16\n",
        "def make_env():\n",
        "  def _thunk():\n",
        "    env = MiniPacman(mode,1000)\n",
        "    return env\n",
        "  return _thunk\n",
        "envs = [make_env() for i in range(num_envs)]\n",
        "envs = SubprocVecEnv(envs)\n",
        "state_shape = envs.observation_space.shape"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
            "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
            "/usr/local/lib/python3.6/dist-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
            "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
            "/usr/local/lib/python3.6/dist-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
            "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
            "/usr/local/lib/python3.6/dist-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
            "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
            "/usr/local/lib/python3.6/dist-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
            "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
            "/usr/local/lib/python3.6/dist-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
            "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
            "/usr/local/lib/python3.6/dist-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
            "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
            "/usr/local/lib/python3.6/dist-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
            "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
            "/usr/local/lib/python3.6/dist-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
            "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
            "/usr/local/lib/python3.6/dist-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
            "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
            "/usr/local/lib/python3.6/dist-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
            "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
            "/usr/local/lib/python3.6/dist-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
            "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
            "/usr/local/lib/python3.6/dist-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
            "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
            "/usr/local/lib/python3.6/dist-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
            "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
            "/usr/local/lib/python3.6/dist-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
            "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
            "/usr/local/lib/python3.6/dist-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
            "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ADPgtFi1CvbT",
        "colab_type": "text"
      },
      "source": [
        "# **Init and Train**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mxDoqyh_C0Qn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gamma = 0.99\n",
        "entropy_coef = 0.01\n",
        "value_loss_coef = 0.5\n",
        "max_grad_norm = 0.5\n",
        "num_steps = 5\n",
        "num_frames = int(10e5)\n",
        "\n",
        "lr = 7e-4\n",
        "eps = 1e-5\n",
        "alpha = 0.99\n",
        "actor_critic = ActorCritic(envs.observation_space.shape,envs.action_space.n)\n",
        "optimizer = optim.RMSprop(actor_critic.parameters(),lr,eps=eps,alpha=alpha)\n",
        "if USE_CUDA:\n",
        "  actor_critic = actor_critic.cuda()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8YysO2S3DlO4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "rollout = RolloutStorage(num_steps, num_envs, envs.observation_space.shape)\n",
        "rollout.cuda()\n",
        "\n",
        "all_rewards = []\n",
        "all_losses  = []"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2wY046poD2DI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "state = envs.reset()\n",
        "state = torch.FloatTensor(np.float32(state))\n",
        "\n",
        "rollout.states[0].copy_(state)\n",
        "episode_rewards = torch.zeros(num_envs,1)\n",
        "final_rewards = torch.zeros(num_envs,1)\n",
        "for i_update in range(num_frames):\n",
        "  for step in range(num_steps):\n",
        "    action = actor_critic.act(Variable(state))\n",
        "    next_state,rewards,done, _= envs.step(action.sequeeze(1).cpu().data.numpy())\n",
        "    reward = torch.FloatTensor(reward).unsequeeze(1)\n",
        "    episode_rewards +=reward\n",
        "    masks = torch.FloatTensor(1-np.array(done)).unsequeeze(1)\n",
        "    final_rewards *= masks\n",
        "    final_rewards += (1-masks) * episode_rewards\n",
        "    episode_rewards *=masks\n",
        "\n",
        "    if USE_CUDA:\n",
        "      masks = masks.cuda()\n",
        "    state = torch.FloatTensor(np.float32(next_state))\n",
        "    rollout.insert(step,state,action.data,reward,mask)\n",
        "  _, next_value = actor_critic(Variable(rollout.states[-1],volatile=True))\n",
        "  next_value = next_value.data\n",
        "  returns = rollout.compute_returns(next_value,gamma)\n",
        "  logit,action_log_probs,values,entropy =actor_critic.evaluate_actions(\n",
        "      Variable(rollout.states[:-1]).view(-1, *state_shape),\n",
        "      Variable(rollout.actions).view(-1,1)\n",
        "  )\n",
        "  values = values.view(num_steps,num_envs,1)\n",
        "  action_log_probs = action_log_probs.view(num_steps,num_envs,1)\n",
        "  advantages = Variable(returns) - values\n",
        "\n",
        "  value_loss = advantages.pow(2).mean()\n",
        "  action_loss = -(Variable(advantages.data)*action_log_probs).mean()\n",
        "\n",
        "  optimizer.zero_grad()\n",
        "  loss = value_loss * value_loss_coef + action_loss - entropy * entropy_coef\n",
        "  loss.backward()\n",
        "  nn.utils.clip_grad_norm(actor_critic.parameters(),max_grad_norm)\n",
        "  optimizer.step()\n",
        "\n",
        "  if i_update % 100 ==0:\n",
        "    all_rewards.append(final_rewards.mean())\n",
        "    all_losses.append(loss.data[0])\n",
        "\n",
        "    clear_output(True)\n",
        "    plt.figure(figsize=(20,5))\n",
        "    plt.subplot(131)\n",
        "    plt.title('epoch %s. reward: %s'% (i_update,np.mean(all_rewards[-10:])))\n",
        "    plt.plot(all_rewards)\n",
        "    plt.subplot(132)\n",
        "    plt.title('loss %s'% all_losses[-1])\n",
        "    plt.plot(all_losses)\n",
        "    plt.show()\n",
        "  rollout.after_update()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tn0V0XFKVMrd",
        "colab_type": "text"
      },
      "source": [
        "# **Saving Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Pr7z4XcVP6P",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 164
        },
        "outputId": "ddac4cbf-f73d-4c39-aae3-c7a0b8cdd4f6"
      },
      "source": [
        "torch.save(actor_critic.state_dict(),\"actor_critic_\" + mode)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-489e548db17d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactor_critic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"actor_critic_\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'actor_critic' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5qhWAXMKVblw",
        "colab_type": "text"
      },
      "source": [
        "# **Let's see the Game**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V3YwoxknVgv3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "def displayImage(image,step,reward):\n",
        "  clear_output(True)\n",
        "  s = \"step: \" + str(step) + \"reward: \" +str(reward)\n",
        "  plt.figure(figsize=(10,3))\n",
        "  plt.title(s)\n",
        "  plt.imshow(image)\n",
        "  plt.show()\n",
        "  time.sleep(0.1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Akt_5mhtV82o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "env = MiniPacman(mode,1000)\n",
        "done =False\n",
        "state = env.reset()\n",
        "total_reward = 0\n",
        "step = 1\n",
        "\n",
        "while not done:\n",
        "  current_state = torch.FloatTensor(state).unsequeeze(0)\n",
        "  if USE_CUDA:\n",
        "    current_state = current_state.cuda()\n",
        "  action = actor_critic.act(Variable(current_state))\n",
        "  next_state,reward,done,_ = env.step(action.data[0,0])\n",
        "  total_reward +=reward\n",
        "  state = next_state\n",
        "\n",
        "  image = torch.FloatTensor(state).permute(1,2,0).cpu().numpy()\n",
        "  displayImage(image,step,total_reward)\n",
        "  step +=1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AUX3Z5S8aY5A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "from IPython.display import clear_output\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vG10-zooagww",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "USE_CUDA = torch.cuda.is_available()\n",
        "Variable = lambda *args, **kwargs: autograd.Variable(*args,**kwargs).cuda() if USE_CUDA else autograd.Variable(*args,**kwargs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tv1Kp0xGa5fl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pixels = (\n",
        "    (0.0,1.0,1.0),\n",
        "    (0.0,1.0,0.0),\n",
        "    (0.0,0.0,1.0),\n",
        "    (1.0,1.0,1.0),\n",
        "    (1.0,1.0,0.0),\n",
        "    (0.0,0.0,0.0),\n",
        "    (1.0,0.0,0.0),\n",
        ")\n",
        "pixel_to_categorical = {pix:i for i, pix in enumerate(pixels)}\n",
        "num_pixels = len(pixels)\n",
        "model_rewards = {\n",
        "    \"regular\" :[0,1,2,3,4,5,6,7,8,9],\n",
        "    \"avoid\"   :[0.1,-0.1,-5,-10,-20],\n",
        "    \"hunt\"    :[0,1,10,-20],\n",
        "    \"ambush\"  :[0,-0.1,10,-20],\n",
        "    \"rush\"    :[0,-0.1,9.9]\n",
        "}\n",
        "reward_to_categorical = {mode : {reward:i for i, reward in enumerate(mode_rewards[mode])}for mode in \n",
        "                         mode_rewards.keys()}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HDRWzhZ_cY2e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def pix_to_target(next_states):\n",
        "  target =[]\n",
        "  for pixel in next_states.transpose(0,2,3,1).reshape(-1,3):\n",
        "    target.append(pixel_to_categorical[tuple([np.ceil(pixel[0]),np.ceil(pixel[1]),np.ceil(pixel[2])])])\n",
        "  return target\n",
        "def target_to_pix(imagined_states):\n",
        "  pixels =[]\n",
        "  to_pixel = {value: key for key , value in pixel_to_categorical.items()}\n",
        "  for target in imagined_states:\n",
        "    pixels.append(list(to_pixel[target]))\n",
        "  return np.array(pixels)\n",
        "def rewards_to_target(mode,rewards):\n",
        "  targets = []\n",
        "  for reward in rewards:\n",
        "    target.append(reward_to_categorical[mode][reward])\n",
        "  return target\n",
        "def plot(frame_idx,rewards,losses):\n",
        "  clear_output(True)\n",
        "  plt.figure(figsize=(20,5))\n",
        "  plt.subplot(131)\n",
        "  plt.title('loss %s' % losses[-1])\n",
        "  plt.plot(losses)\n",
        "  plt.show()\n",
        "    \n",
        "def displayImage(image, step, reward):\n",
        "  s = str(step) + \" \" + str(reward)\n",
        "  plt.title(s)\n",
        "  plt.imshow(image)\n",
        "  plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_IDfXMYlcZmS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BasicBlock(nn.Module):\n",
        "  def __init__(self,in_shape,n1,n2,n3):\n",
        "    super(BasicBlock,self).__init__()\n",
        "\n",
        "    self.in_shape = in_shape\n",
        "    self.n1 = n1\n",
        "    self.n2 = n2\n",
        "    self.n3 = n3\n",
        "    self.maxpool = nn.MaxPool2d(kernel_size=in_shape[1:])\n",
        "    self.conv1 = nn.Sequential(\n",
        "        nn.Conv2d(in_shape[0]* 2, n1,kernel_size=1, stride = 2,padding=6),\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(n1,n1,kernel_size=10,stride = 1, padding=(5,6)),\n",
        "        nn.ReLU(),\n",
        "    )\n",
        "    self.conv2 = nn.Sequential(\n",
        "        nn.Conv2d(in_shape[0] *2,n2,kernel_size=1),\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(n2,n2,kernel_size=3,stride =1,padding=1),\n",
        "        nn.ReLU(),\n",
        "    )\n",
        "    self.conv3 = nn.Sequential(\n",
        "        nn.Conv2d(n1 + n2,n3,kernel_size=1),\n",
        "        nn.ReLU(),\n",
        "    )\n",
        "  def forward(self,inputs):\n",
        "    x = self.pool_and_inject(inputs)\n",
        "    x = torch.cat([self.conv1(x),self.conv2(x)],1)\n",
        "    x = self.conv3(x)\n",
        "    x = torch.cat([x,inputs],1)\n",
        "  def pool_and_inject(self,x):\n",
        "    pooled = self.maxpool(x)\n",
        "    tiled = pooled.expand((x.size(0),)+ self.in_shape)\n",
        "    out = torch.cat([tiled,x],1)\n",
        "    return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AI6OPaUAcZ7F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class EnvModel(nn.Module):\n",
        "  def __init__(self,in_shape,num_pixels,num_rewards):\n",
        "    super(EnvModel,self).__init___()\n",
        "    width = in_shape[1]\n",
        "    height = in_shape[2]\n",
        "    self.conv = nn.Sequential(\n",
        "        nn.Conv2d(8,64,kernel_size=1),\n",
        "        nn.ReLU()\n",
        "    )\n",
        "    self.basic_block1 = BasicBlock((64,width,height),16,32,64)\n",
        "    self.basic_block2 = BasicBlock((128,width,height),16,32,64)\n",
        "    self.image_conv = nn.Sequential(\n",
        "        nn.Conv2d(192,256,kernel_size=1),\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(64,64,kernel_size=1),\n",
        "        nn.ReLU()\n",
        "    )\n",
        "    self.image_fc = nn.Linear(256,num_pixels)\n",
        "    self.reward_conv = nn.Sequential(\n",
        "        nn.Conv2d(192,64,kernel_size=1),\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(64,64,kernel_size=1),\n",
        "        nn.ReLU()\n",
        "    )\n",
        "    self.reward_fc = nn.Linear(64 * width * height, num_rewards)\n",
        "  def forward(self,inputs):\n",
        "    batch_size = input.size(0)\n",
        "    x = self.conv(inputs)\n",
        "    x = self.basic_block1(x)\n",
        "    x = self.basic_block2(x)\n",
        "    image = self.image_conv(x)\n",
        "    image = image.permute(0,2,3,1).contiguous().view(-1,256)\n",
        "    image = self.image_fc(image)\n",
        "\n",
        "    reward = self.reward_conv(x)\n",
        "    reward = reward.view(batch_size,-1)\n",
        "    reward = self.reward_fc(reward)\n",
        "    return image,reward"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e45JxJI6ie0t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mode = \"regular\"\n",
        "num_envs = 16\n",
        "def make_env():\n",
        "  def _thunk():\n",
        "    env = MiniPacman(mode,1000)\n",
        "  return _thunk\n",
        "envs = [make_env() for i in range(num_envs)]\n",
        "envs = SubprocVecEnv(envs)\n",
        "state_shape = envs.observation_space.shape\n",
        "num_actions = envs.actions_space.n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a5VXC8EnjKFL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "env_model    = EnvModel(envs.observation_space.shape, num_pixels, len(mode_rewards[\"regular\"]))\n",
        "actor_critic = ActorCritic(envs.observation_space.shape, envs.action_space.n)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(env_model.parameters())\n",
        "\n",
        "if USE_CUDA:\n",
        "    env_model    = env_model.cuda()\n",
        "    actor_critic = actor_critic.cuda()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Za5KcRaRjKre",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "actor_critic.load_state_dict(torch.load(\"actor_critic_\" + mode))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uynR7XbzjLMN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_action(state):\n",
        "    if state.ndim == 4:\n",
        "        state = torch.FloatTensor(np.float32(state))\n",
        "    else:\n",
        "        state = torch.FloatTensor(np.float32(state)).unsqueeze(0)\n",
        "        \n",
        "    action = actor_critic.act(Variable(state, volatile=True))\n",
        "    action = action.data.cpu().squeeze(1).numpy()\n",
        "    return action"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZQMH26yajYs1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def play_games(envs, frames):\n",
        "    states = envs.reset()\n",
        "    \n",
        "    for frame_idx in range(frames):\n",
        "        actions = get_action(states)\n",
        "        next_states, rewards, dones, _ = envs.step(actions)\n",
        "        \n",
        "        yield frame_idx, states, actions, rewards, next_states, dones\n",
        "        \n",
        "        states = next_states"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rin35Cs_jbUn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "reward_coef = 0.1\n",
        "num_updates = 5000\n",
        "\n",
        "losses = []\n",
        "all_rewards = []\n",
        "\n",
        "for frame_idx, states, actions, rewards, next_states, dones in play_games(envs, num_updates):\n",
        "    states      = torch.FloatTensor(states)\n",
        "    actions     = torch.LongTensor(actions)\n",
        "\n",
        "    batch_size = states.size(0)\n",
        "    \n",
        "    onehot_actions = torch.zeros(batch_size, num_actions, *state_shape[1:])\n",
        "    onehot_actions[range(batch_size), actions] = 1\n",
        "    inputs = Variable(torch.cat([states, onehot_actions], 1))\n",
        "    \n",
        "    if USE_CUDA:\n",
        "        inputs = inputs.cuda()\n",
        "\n",
        "    imagined_state, imagined_reward = env_model(inputs)\n",
        "\n",
        "    target_state = pix_to_target(next_states)\n",
        "    target_state = Variable(torch.LongTensor(target_state))\n",
        "    \n",
        "    target_reward = rewards_to_target(mode, rewards)\n",
        "    target_reward = Variable(torch.LongTensor(target_reward))\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    image_loss  = criterion(imagined_state, target_state)\n",
        "    reward_loss = criterion(imagined_reward, target_reward)\n",
        "    loss = image_loss + reward_coef * reward_loss\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    \n",
        "    losses.append(loss.data[0])\n",
        "    all_rewards.append(np.mean(rewards))\n",
        "    \n",
        "    if frame_idx % 10 == 0:\n",
        "        plot(frame_idx, all_rewards, losses)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ffp9jBwvjblR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.save(env_model.state_dict(), \"env_model_\" + mode)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HQ0aVsH3jbzR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "\n",
        "env = MiniPacman(mode, 1000)\n",
        "batch_size = 1\n",
        "\n",
        "done = False\n",
        "state = env.reset()\n",
        "iss = []\n",
        "ss  = []\n",
        "\n",
        "steps = 0\n",
        "\n",
        "while not done:\n",
        "    steps += 1\n",
        "    actions = get_action(state)\n",
        "    onehot_actions = torch.zeros(batch_size, num_actions, *state_shape[1:])\n",
        "    onehot_actions[range(batch_size), actions] = 1\n",
        "    state = torch.FloatTensor(state).unsqueeze(0)\n",
        "    \n",
        "    inputs = Variable(torch.cat([state, onehot_actions], 1))\n",
        "    if USE_CUDA:\n",
        "        inputs = inputs.cuda()\n",
        "\n",
        "    imagined_state, imagined_reward = env_model(inputs)\n",
        "    imagined_state = F.softmax(imagined_state)\n",
        "    iss.append(imagined_state)\n",
        "    \n",
        "    next_state, reward, done, _ = env.step(actions[0])\n",
        "    ss.append(state)\n",
        "    state = next_state\n",
        "    \n",
        "    imagined_image = target_to_pix(imagined_state.view(batch_size, -1, len(pixels))[0].max(1)[1].data.cpu().numpy())\n",
        "    imagined_image = imagined_image.reshape(15, 19, 3)\n",
        "    state_image = torch.FloatTensor(next_state).permute(1, 2, 0).cpu().numpy()\n",
        "    \n",
        "    clear_output()\n",
        "    plt.figure(figsize=(10,3))\n",
        "    plt.subplot(131)\n",
        "    plt.title(\"Imagined\")\n",
        "    plt.imshow(imagined_image)\n",
        "    plt.subplot(132)\n",
        "    plt.title(\"Actual\")\n",
        "    plt.imshow(state_image)\n",
        "    plt.show()\n",
        "    time.sleep(0.3)\n",
        "    \n",
        "    if steps > 30:\n",
        "        break"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Ps7SKXujcBn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from IPython.display import clear_output\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3NgylI86jcOW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "USE_CUDA = torch.cuda.is_available()\n",
        "Variable = lambda *args, **kwargs: autograd.Variable(*args, **kwargs).cuda() if USE_CUDA else autograd.Variable(*args, **kwargs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h9z7eiSljcoQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pixels = (\n",
        "    (0.0, 1.0, 0.0), \n",
        "    (0.0, 1.0, 1.0),\n",
        "    (0.0, 0.0, 1.0),\n",
        "    (1.0, 1.0, 1.0),\n",
        "    (1.0, 1.0, 0.0), \n",
        "    (0.0, 0.0, 0.0),\n",
        "    (1.0, 0.0, 0.0)\n",
        ")\n",
        "pixel_to_onehot = {pix:i for i, pix in enumerate(pixels)} \n",
        "num_pixels = len(pixels)\n",
        "\n",
        "task_rewards = {\n",
        "    \"regular\": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\n",
        "    \"avoid\":   [0.1, -0.1, -5, -10, -20],\n",
        "    \"hunt\":    [0, 1, 10, -20],\n",
        "    \"ambush\":  [0, -0.1, 10, -20],\n",
        "    \"rush\":    [0, -0.1, 9.9]\n",
        "}\n",
        "reward_to_onehot = {mode: {reward:i for i, reward in enumerate(task_rewards[mode])} for mode in task_rewards.keys()}\n",
        "\n",
        "def pix_to_target(next_states):\n",
        "    target = []\n",
        "    for pixel in next_states.transpose(0, 2, 3, 1).reshape(-1, 3):\n",
        "        target.append(pixel_to_onehot[tuple([np.round(pixel[0]), np.round(pixel[1]), np.round(pixel[2])])])\n",
        "    return target\n",
        "\n",
        "def target_to_pix(imagined_states):\n",
        "    pixels = []\n",
        "    to_pixel = {value: key for key, value in pixel_to_onehot.items()}\n",
        "    for target in imagined_states:\n",
        "        pixels.append(list(to_pixel[target]))\n",
        "    return np.array(pixels)\n",
        "\n",
        "def rewards_to_target(mode, rewards):\n",
        "    target = []\n",
        "    for reward in rewards:\n",
        "        target.append(reward_to_onehot[mode][reward])\n",
        "    return target\n",
        "    \n",
        "def displayImage(image, step, reward):\n",
        "    s = str(step) + \" \" + str(reward)\n",
        "    plt.title(s)\n",
        "    plt.imshow(image)\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L722UK-Pjc2P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mode = \"regular\"\n",
        "num_envs = 16\n",
        "\n",
        "def make_env():\n",
        "    def _thunk():\n",
        "        env = MiniPacman(mode, 1000)\n",
        "        return env\n",
        "\n",
        "    return _thunk\n",
        "\n",
        "envs = [make_env() for i in range(num_envs)]\n",
        "envs = SubprocVecEnv(envs)\n",
        "\n",
        "state_shape = envs.observation_space.shape\n",
        "num_actions = envs.action_space.n\n",
        "num_rewards = len(task_rewards[mode])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eJT-BmMvkXQQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class RolloutEncoder(nn.Module):\n",
        "  def __init__(self,in_shape,num_rewards,hidden_size):\n",
        "    super(RolloutEncoder,self).__init__()\n",
        "    self.in_shape = in_shape \n",
        "    self.features = nn.Sequential(\n",
        "        nn.Conv2d(in_shape[0],16,kernel_size=3,stride=1),\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(16,16,kernel_size=3,stride=2),\n",
        "        nn.ReLU(),\n",
        "    )\n",
        "    self.gru = nn.GRU(self.feature_size()+ num_rewards,hidden_size)\n",
        "  def forward(self,state,reward):\n",
        "    num_steps = state.size(0)\n",
        "    batch_size = state.size(1)\n",
        "    state = state.view(-1,*self.in_shape)\n",
        "    state = self.features(state)\n",
        "    state = state.view(num_steps,batch_size,-1)\n",
        "    rnn_input = torch.cat([state,reward],2)\n",
        "    _, hidden = self.gru(rnn_input)\n",
        "    return hidden.sequeeze(0)\n",
        "  def feature_size(self):\n",
        "    return self.features(autograd.Variable(torch.zeros(1, *self.in_shape))).view(1,-1).size(1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dc0yZsx4l6vP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class I2A(OnPolicy):\n",
        "  def __init__(self,in_shape,num_actions,num_rewards,hidden_size,imagination,full_rollout = True):\n",
        "    super(I2A,self).__init__()\n",
        "    self.in_shape = in_shape\n",
        "    self.num_actions = num_actions\n",
        "    self.num_rewards = num_rewards\n",
        "    self.imagination = imagination\n",
        "    self.features = nn.Sequential(\n",
        "        nn.Conv2d(in_shape[0],16,kernel_size=3,stride=1),\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(16,16,kernel_size=3,stride=2),\n",
        "        nn.ReLU(),\n",
        "    )\n",
        "    self.encoder = RolloutEncoder(in_shape,num_rewards,hidden_size)\n",
        "    if full_rollout:\n",
        "      self.fc = nn.Sequential(\n",
        "          nn.Linear(self.feature_size() + num_actions * hidden_size, 256),\n",
        "          nn.ReLU(),\n",
        "      )\n",
        "    else:\n",
        "      self.fc = nn.Sequential(\n",
        "          nn.Linear(self.feature_size() + hidden_size,256),\n",
        "          nn.ReLU(),\n",
        "      )\n",
        "    self.critic = nn.Linear(256,1)\n",
        "    self.actor = nn.Linear(256,num_actions)\n",
        "  def forward(self,state):\n",
        "    batch_size = state.size(0)\n",
        "    imagined_state , imagined_reward = self.imagination(state.data)\n",
        "    hidden = self.encoder(Variable(imagined_state),Variable(imagined_reward))\n",
        "    state = self.features(state)\n",
        "    state = state.view(state.size(0),-1)\n",
        "    x = torch.cat([state,hidden],1)\n",
        "    x = self.fc(x)\n",
        "    logit = self.actor(x)\n",
        "    value = self.critic(x)\n",
        "\n",
        "    return logit,value\n",
        "  def feature_size(self):\n",
        "    return self.features(autograd.Variable(torch.zeros(1, *self.in_shape))).view(1,-1).size(1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jkDAleeipEgq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ImaginationCore(object):\n",
        "    def __init__(self, num_rolouts, in_shape, num_actions, num_rewards, env_model, distil_policy, full_rollout=True):\n",
        "        self.num_rolouts  = num_rolouts\n",
        "        self.in_shape      = in_shape\n",
        "        self.num_actions   = num_actions\n",
        "        self.num_rewards   = num_rewards\n",
        "        self.env_model     = env_model\n",
        "        self.distil_policy = distil_policy\n",
        "        self.full_rollout  = full_rollout\n",
        "        \n",
        "    def __call__(self, state):\n",
        "        state      = state.cpu()\n",
        "        batch_size = state.size(0)\n",
        "\n",
        "        rollout_states  = []\n",
        "        rollout_rewards = []\n",
        "\n",
        "        if self.full_rollout:\n",
        "            state = state.unsqueeze(0).repeat(self.num_actions, 1, 1, 1, 1).view(-1, *self.in_shape)\n",
        "            action = torch.LongTensor([[i] for i in range(self.num_actions)]*batch_size)\n",
        "            action = action.view(-1)\n",
        "            rollout_batch_size = batch_size * self.num_actions\n",
        "        else:\n",
        "            action = self.distil_policy.act(Variable(state, volatile=True))\n",
        "            action = action.data.cpu()\n",
        "            rollout_batch_size = batch_size\n",
        "\n",
        "        for step in range(self.num_rolouts):\n",
        "            onehot_action = torch.zeros(rollout_batch_size, self.num_actions, *self.in_shape[1:])\n",
        "            onehot_action[range(rollout_batch_size), action] = 1\n",
        "            inputs = torch.cat([state, onehot_action], 1)\n",
        "\n",
        "            imagined_state, imagined_reward = self.env_model(Variable(inputs, volatile=True))\n",
        "\n",
        "            imagined_state  = F.softmax(imagined_state).max(1)[1].data.cpu()\n",
        "            imagined_reward = F.softmax(imagined_reward).max(1)[1].data.cpu()\n",
        "\n",
        "            imagined_state = target_to_pix(imagined_state.numpy())\n",
        "            imagined_state = torch.FloatTensor(imagined_state).view(rollout_batch_size, *self.in_shape)\n",
        "\n",
        "            onehot_reward = torch.zeros(rollout_batch_size, self.num_rewards)\n",
        "            onehot_reward[range(rollout_batch_size), imagined_reward] = 1\n",
        "\n",
        "            rollout_states.append(imagined_state.unsqueeze(0))\n",
        "            rollout_rewards.append(onehot_reward.unsqueeze(0))\n",
        "\n",
        "            state  = imagined_state\n",
        "            action = self.distil_policy.act(Variable(state, volatile=True))\n",
        "            action = action.data.cpu()\n",
        "        \n",
        "        return torch.cat(rollout_states), torch.cat(rollout_rewards)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9_FXkBhxpE5x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "full_rollout = True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EGFeL0GbpFQn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "env_model     = EnvModel(envs.observation_space.shape, num_pixels, num_rewards)\n",
        "env_model.load_state_dict(torch.load(\"env_model_\" + mode))\n",
        "\n",
        "distil_policy = ActorCritic(envs.observation_space.shape, envs.action_space.n)\n",
        "distil_optimizer = optim.Adam(distil_policy.parameters())\n",
        "\n",
        "imagination = ImaginationCore(1, state_shape, num_actions, num_rewards, env_model, distil_policy, full_rollout=full_rollout)\n",
        "\n",
        "actor_critic = I2A(state_shape, num_actions, num_rewards, 256, imagination, full_rollout=full_rollout)\n",
        "#rmsprop hyperparams:\n",
        "lr    = 7e-4\n",
        "eps   = 1e-5\n",
        "alpha = 0.99\n",
        "optimizer = optim.RMSprop(actor_critic.parameters(), lr, eps=eps, alpha=alpha)\n",
        "\n",
        "\n",
        "if USE_CUDA:\n",
        "    env_model     = env_model.cuda()\n",
        "    distil_policy = distil_policy.cuda()\n",
        "    actor_critic  = actor_critic.cuda()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xK-hiPSzwVnx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gamma = 0.99\n",
        "entropy_coef = 0.01\n",
        "value_loss_coef = 0.5\n",
        "max_grad_norm = 0.5\n",
        "num_steps = 5\n",
        "num_frames = int(10e5)\n",
        "\n",
        "rollout = RolloutStorage(num_steps, num_envs, envs.observation_space.shape)\n",
        "rollout.cuda()\n",
        "\n",
        "all_rewards = []\n",
        "all_losses  = []"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T3cWyMnywV59",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "state = envs.reset()\n",
        "current_state = torch.FloatTensor(np.float32(state))\n",
        "\n",
        "rollout.states[0].copy_(current_state)\n",
        "\n",
        "episode_rewards = torch.zeros(num_envs, 1)\n",
        "final_rewards   = torch.zeros(num_envs, 1)\n",
        "\n",
        "for i_update in range(num_frames):\n",
        "\n",
        "    for step in range(num_steps):\n",
        "        if USE_CUDA:\n",
        "            current_state = current_state.cuda()\n",
        "        action = actor_critic.act(Variable(current_state))\n",
        "\n",
        "        next_state, reward, done, _ = envs.step(action.squeeze(1).cpu().data.numpy())\n",
        "\n",
        "        reward = torch.FloatTensor(reward).unsqueeze(1)\n",
        "        episode_rewards += reward\n",
        "        masks = torch.FloatTensor(1-np.array(done)).unsqueeze(1)\n",
        "        final_rewards *= masks\n",
        "        final_rewards += (1-masks) * episode_rewards\n",
        "        episode_rewards *= masks\n",
        "\n",
        "        if USE_CUDA:\n",
        "            masks = masks.cuda()\n",
        "\n",
        "        current_state = torch.FloatTensor(np.float32(next_state))\n",
        "        rollout.insert(step, current_state, action.data, reward, masks)\n",
        "\n",
        "\n",
        "    _, next_value = actor_critic(Variable(rollout.states[-1], volatile=True))\n",
        "    next_value = next_value.data\n",
        "\n",
        "    returns = rollout.compute_returns(next_value, gamma)\n",
        "\n",
        "    logit, action_log_probs, values, entropy = actor_critic.evaluate_actions(\n",
        "        Variable(rollout.states[:-1]).view(-1, *state_shape),\n",
        "        Variable(rollout.actions).view(-1, 1)\n",
        "    )\n",
        "    \n",
        "    distil_logit, _, _, _ = distil_policy.evaluate_actions(\n",
        "        Variable(rollout.states[:-1]).view(-1, *state_shape),\n",
        "        Variable(rollout.actions).view(-1, 1)\n",
        "    )\n",
        "        \n",
        "    distil_loss = 0.01 * (F.softmax(logit).detach() * F.log_softmax(distil_logit)).sum(1).mean()\n",
        "\n",
        "    values = values.view(num_steps, num_envs, 1)\n",
        "    action_log_probs = action_log_probs.view(num_steps, num_envs, 1)\n",
        "    advantages = Variable(returns) - values\n",
        "\n",
        "    value_loss = advantages.pow(2).mean()\n",
        "    action_loss = -(Variable(advantages.data) * action_log_probs).mean()\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss = value_loss * value_loss_coef + action_loss - entropy * entropy_coef\n",
        "    loss.backward()\n",
        "    nn.utils.clip_grad_norm(actor_critic.parameters(), max_grad_norm)\n",
        "    optimizer.step()\n",
        "    \n",
        "    distil_optimizer.zero_grad()\n",
        "    distil_loss.backward()\n",
        "    optimizer.step()\n",
        "    \n",
        "    if i_update % 100 == 0:\n",
        "        all_rewards.append(final_rewards.mean())\n",
        "        all_losses.append(loss.data[0])\n",
        "        \n",
        "        clear_output(True)\n",
        "        plt.figure(figsize=(20,5))\n",
        "        plt.subplot(131)\n",
        "        plt.title('epoch %s. reward: %s' % (i_update, np.mean(all_rewards[-10:])))\n",
        "        plt.plot(all_rewards)\n",
        "        plt.subplot(132)\n",
        "        plt.title('loss %s' % all_losses[-1])\n",
        "        plt.plot(all_losses)\n",
        "        plt.show()\n",
        "        \n",
        "    rollout.after_update()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3SIfNCdIwWLb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.save(actor_critic.state_dict(), \"i2a_\" + mode)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}